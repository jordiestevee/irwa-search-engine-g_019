{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDmD8ZIjlpoa",
        "outputId": "197bfc49-b678-4cf3-d1b4-d745e7b4ce22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import wordcloud\n",
        "from wordcloud import WordCloud\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from array import array\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import math\n",
        "import numpy as np\n",
        "import collections\n",
        "from numpy import linalg as la\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FngiI3uUm1lX",
        "outputId": "62237917-65f9-4257-e9d6-0029091d8d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQePC8-lnKSB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "7e2645fb-d468-46b3-b7b3-72c11812d64d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'fashion_products_dataset.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-408219246.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_json_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fashion_products_dataset.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-408219246.py\u001b[0m in \u001b[0;36miter_json_objects\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miter_json_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.gz'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fashion_products_dataset.json'"
          ]
        }
      ],
      "source": [
        "#DATA LOADING\n",
        "\n",
        "import json, gzip\n",
        "from pathlib import Path\n",
        "\n",
        "def iter_json_objects(path):\n",
        "    opener = gzip.open if str(path).endswith('.gz') else open\n",
        "    with opener(path, 'rt', encoding='utf-8') as f:\n",
        "        first = f.read(1)\n",
        "        f.seek(0)\n",
        "        if first == '[':\n",
        "            # big array\n",
        "            try:\n",
        "                import ijson\n",
        "                for obj in ijson.items(f, 'item'):\n",
        "                    yield obj\n",
        "            except ImportError:\n",
        "                data = json.load(f)  # may be heavy\n",
        "                for obj in data:\n",
        "                    yield obj\n",
        "        else:\n",
        "            # NDJSON\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    yield json.loads(line)\n",
        "\n",
        "# usage\n",
        "records = list(iter_json_objects('fashion_products_dataset.json'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LdmVKUYnYoQ"
      },
      "outputs": [],
      "source": [
        "# === Minimal preprocessing with NLTK tokenizer (Option A) =====================\n",
        "# Run this cell once. If re-running, downloads will be no-ops.\n",
        "\n",
        "#DATA PREPARATION\n",
        "\n",
        "import re, numpy as np, nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure NLTK data is available (quiet if already downloaded)\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except LookupError:\n",
        "    nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "# Some environments also need this; harmless if not required\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
        "except LookupError:\n",
        "    try:\n",
        "        nltk.download(\"punkt_tab\", quiet=True)\n",
        "    except Exception:\n",
        "        pass  # not present on all NLTK builds\n",
        "\n",
        "try:\n",
        "    nltk.data.find(\"corpora/stopwords\")\n",
        "except LookupError:\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer     = PorterStemmer()\n",
        "\n",
        "def _num(x):\n",
        "    if x is None: return np.nan\n",
        "    if isinstance(x,(int,float)): return float(x)\n",
        "    s = str(x).replace(\",\",\"\").strip().lower()\n",
        "    if s.endswith(\"% off\"): s = s.split(\"%\")[0]\n",
        "    if s in (\"true\",\"false\"): return float(s==\"true\")\n",
        "    try: return float(s)\n",
        "    except: return np.nan\n",
        "\n",
        "def _details_to_text(dets):\n",
        "    if not dets: return \"\"\n",
        "    parts = []\n",
        "    for d in dets:\n",
        "        if isinstance(d, dict):\n",
        "            for k,v in d.items():\n",
        "                parts.append(f\"{k} {v}\")\n",
        "    return \" \".join(parts)\n",
        "\n",
        "def product_processor(rec):\n",
        "    # 1) gather text\n",
        "    title   = rec.get(\"title\",\"\")\n",
        "    desc    = rec.get(\"description\",\"\")\n",
        "    details = _details_to_text(rec.get(\"product_details\"))\n",
        "    raw_txt = f\"{title} {desc} {details}\"\n",
        "\n",
        "    # 2) tokenize -> lowercase -> keep alnum -> stopwords -> stem\n",
        "    toks = word_tokenize(raw_txt)\n",
        "    toks = [t.lower() for t in toks if t.isalnum()]\n",
        "    toks = [stemmer.stem(t) for t in toks if t not in stop_words and len(t) > 2]\n",
        "    cleaned = \" \".join(toks)\n",
        "\n",
        "    # 3) return processed row (preserve all required fields)\n",
        "    return {\n",
        "        \"pid\": rec.get(\"pid\",\"\"),\n",
        "        \"title\": title,\n",
        "        \"description\": desc,\n",
        "        \"brand\": rec.get(\"brand\",\"\"),\n",
        "        \"category\": rec.get(\"category\",\"\"),\n",
        "        \"sub_category\": rec.get(\"sub_category\",\"\"),\n",
        "        \"product_details\": rec.get(\"product_details\") or [],\n",
        "        \"seller\": rec.get(\"seller\",\"\"),\n",
        "        \"out_of_stock\": _num(rec.get(\"out_of_stock\")),\n",
        "        \"selling_price\": _num(rec.get(\"selling_price\")),\n",
        "        \"discount\": _num(rec.get(\"discount\")),\n",
        "        \"actual_price\": _num(rec.get(\"actual_price\")),\n",
        "        \"average_rating\": _num(rec.get(\"average_rating\")),\n",
        "        \"url\": rec.get(\"url\",\"\"),\n",
        "        \"tokens\": cleaned\n",
        "    }\n",
        "# ============================================================================\n",
        "\n",
        "# Example usage (assumes you already defined iter_json_objects(...)):\n",
        "# processed_rows = []\n",
        "# for rec in iter_json_objects(\"fashion_products_dataset.json\"):\n",
        "#     processed_rows.append(product_processor(rec))\n",
        "# import pandas as pd\n",
        "# pd.DataFrame(processed_rows).to_csv(\"cache/processed_products.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process -> list of dicts\n",
        "processed_data = [product_processor(rec) for rec in iter_json_objects(\"fashion_products_dataset.json\")]\n",
        "\n",
        "# DataFrame in one go\n",
        "products_df = pd.DataFrame(processed_data)\n",
        "\n",
        "# Use product ID as index\n",
        "products_df.set_index(\"pid\", inplace=True)\n",
        "\n",
        "# Preview like they did\n",
        "products_df.head(4)\n"
      ],
      "metadata": {
        "id": "2pZLzlznuFX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Product text length plot\n",
        "\n",
        "# Aux column for the length of each\n",
        "vocab_df = products_df.copy()\n",
        "vocab_df['Wordcount'] = products_df['tokens'].fillna('').apply(lambda x: len(x.split()))\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(vocab_df['Wordcount'], bins=22, kde=True)\n",
        "\n",
        "plt.title('Product text length plot')\n",
        "plt.xlabel('Number of words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BCJJldPJuJMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting info about our vocabulary (products)\n",
        "\n",
        "# All product tokens in a single list of words\n",
        "whole_content = products_df['tokens'].fillna('').str.split().explode().tolist()\n",
        "\n",
        "# Counting appearances\n",
        "word_counts = Counter(whole_content)\n",
        "\n",
        "# Converting to dict\n",
        "vocab_dict = dict(word_counts)\n",
        "print('Product vocabulary size is', len(vocab_dict))\n",
        "\n",
        "# Sorting dictionary items to get the top 5 appearing words\n",
        "sorted_dict = sorted(vocab_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "print('\\nThe top 5 most appearing words are:')\n",
        "for item in sorted_dict[:5]:\n",
        "    print(item)\n"
      ],
      "metadata": {
        "id": "K-a-f95Zu0Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a word cloud for product tokens\n",
        "words_wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(vocab_dict)\n",
        "\n",
        "plt.figure(figsize=(14,8))\n",
        "plt.imshow(words_wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GRXz72qRvCYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new dataframe sorted by highest rating\n",
        "rated_df = products_df.copy()\n",
        "rated_sorted = rated_df.sort_values(by='average_rating', ascending=False)\n",
        "\n",
        "print('The following are the Top 5 Highest-Rated Products:')\n",
        "rated_sorted.reset_index().drop(columns=['pid','url'], errors='ignore').head(5)\n"
      ],
      "metadata": {
        "id": "wSlNT4M1vGuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_brands = df['brand'].value_counts().head(10)\n",
        "top_sellers = df['seller'].value_counts().head(10)\n"
      ],
      "metadata": {
        "id": "-cK9q594vcWb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}